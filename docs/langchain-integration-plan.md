# LangChain Integration Plan for Agentic Soup

## Overview
This document outlines the step-by-step plan to integrate LangChain.js into the Agentic Soup project to replace mock tool implementations with real LLM-powered capabilities.

## Current State
- **Agent Architecture**: Plan â†’ Act â†’ Reflect â†’ Learn pattern
- **LLM Integration**: Already has `llmProvider` supporting local (Ollama) and Vertex AI
- **Tools**: Mostly mock implementations
  - `stringKit`: Simple text truncation for summarization
  - `calc`: Unsafe eval() for math
  - `retrieval`: Hardcoded knowledge base
  - `browser`: Basic web scraping without intelligence
- **Planner**: LLMPlanner with fallback to MockPlanner

## Integration Phases

### Phase 1: Tool Enhancement with LangChain (Priority)

#### Step 1.1: Install Dependencies
```bash
# In packages/agents/
pnpm add @langchain/core @langchain/community langchain mathjs
```

#### Step 1.2: Create LangChain LLM Wrapper
**File**: `packages/agents/src/langchainProvider.ts`
- Wrap existing `llmProvider` as LangChain BaseLLM
- Support both local (Ollama) and Vertex AI
- Maintain token tracking and budgets

#### Step 1.3: Enhance stringKit Tool
**File**: `packages/agents/src/tools/langchainStringKit.ts`
- **Summarization**: Use LangChain's summarization chains
- **Classification**: Use LangChain's classification chains
- Configuration:
  ```typescript
  interface StringKitConfig {
    useLLM: boolean;  // Toggle between mock and LLM
    model?: 'local' | 'vertex';
    maxRetries?: number;
  }
  ```

#### Step 1.4: Enhance calc Tool
**File**: `packages/agents/src/tools/langchainCalc.ts`
- Replace eval() with mathjs for safe expression evaluation
- Add LLM validation for complex expressions
- Word problem solving capabilities

#### Step 1.5: Enhance retrieval Tool
**File**: `packages/agents/src/tools/langchainRetrieval.ts`
- Implement vector embeddings for knowledge base
- Use LangChain's retrieval chains
- Support both local KB and external sources

#### Step 1.6: Enhance browser Tool
**File**: `packages/agents/src/tools/langchainBrowser.ts`
- LLM-powered content extraction
- Intelligent navigation decisions
- Structured data extraction from web pages

### Phase 2: LangChain Agent Options

#### Option A: Conservative (Recommended First)
- Keep existing `SimpleAgent` class
- Replace tool implementations only
- Minimal disruption to existing code

#### Option B: Hybrid Implementation
- Create `LangChainAgent` alongside `SimpleAgent`
- Allow configuration to choose agent type
- A/B testing capabilities

#### Option C: Full Migration
- Replace SimpleAgent with LangChain ReActAgent
- Complete rewrite of agent logic
- Maximum LangChain feature utilization

### Phase 3: Advanced Features (Future)

1. **Memory Systems**
   - ConversationSummaryMemory for long-term recall
   - VectorStoreMemory for semantic search
   - Integration with existing agentMemory

2. **Tool Composition**
   - Sequential chains for complex tasks
   - Conditional tool selection
   - Tool output parsing and validation

3. **Observability**
   - LangSmith integration for debugging
   - Token usage tracking
   - Performance metrics

## Implementation Order

### Sprint 1: Foundation âœ… COMPLETED
1. Document plan (this file) âœ…
2. Install LangChain dependencies âœ…
3. Create LangChain LLM wrapper âœ…
4. Implement environment configuration âœ…

### Sprint 2: stringKit Enhancement âœ… COMPLETED  
1. Create LangChain summarization chain âœ…
2. Create LangChain classification chain âœ…
3. Remove fallbacks (graceful failure instead) âœ…
4. Test with existing job types âœ…
5. Fix production bug (failed jobs marked as completed) âœ…

### Sprint 3: calc Tool Security & Enhancement âœ… COMPLETED
1. Replace eval() with mathjs âœ…
2. Add LLM word problem solver âœ…
3. Implement validation and error handling âœ…
4. Test with math job category âœ…
5. Improve LLM prompt for better accuracy âœ…

### Sprint 4: retrieval Tool Intelligence âœ… COMPLETED
1. Set up vector store (in-memory initially) âœ…
2. Implement embedding generation âœ…
3. Create retrieval chain âœ…
4. Test with web_research jobs âœ…

**Note**: The current vector store implementation uses an in-memory LangChain MemoryVectorStore. 
In a future phase, this will be upgraded to use an MCP-based RAG system for more scalable 
and persistent vector storage. The current implementation provides a solid foundation and 
abstraction layer that will make this transition seamless.

### Sprint 5: browser Tool Enhancement
1. Implement LLM-powered content extraction
2. Add intelligent navigation
3. Structure extraction capabilities
4. Integration testing

### Sprint 6: Agent Integration
1. Update SimpleAgent to use new tools
2. Performance comparison (mock vs LLM)
3. Cost analysis
4. Production readiness

## Configuration Strategy

### Environment Variables
```bash
# Enable LangChain tools
LANGCHAIN_ENABLED=true
LANGCHAIN_TOOLS=stringKit,calc,retrieval,browser

# Tool-specific settings
STRINGKIT_USE_LLM=true
CALC_USE_LLM=true
RETRIEVAL_USE_EMBEDDINGS=true
BROWSER_USE_LLM=true

# Fallback behavior
FALLBACK_TO_MOCK=true
MOCK_TIMEOUT_MS=5000
```

### Feature Flags
```typescript
interface LangChainConfig {
  enabled: boolean;
  tools: {
    stringKit: { useLLM: boolean; fallbackToMock: boolean; };
    calc: { useLLM: boolean; useMathJS: boolean; };
    retrieval: { useEmbeddings: boolean; vectorStore: 'memory' | 'file'; };
    browser: { useLLM: boolean; extractionMode: 'simple' | 'structured'; };
  };
  monitoring: {
    trackTokenUsage: boolean;
    logToLangSmith: boolean;
  };
}
```

## Testing Strategy

### Unit Tests
- Test each tool in isolation
- Mock LLM responses for predictable testing
- Verify fallback behavior

### Integration Tests
- Test complete job execution flow
- Compare mock vs LLM outputs
- Performance benchmarks

### Cost Analysis
- Track tokens per job type
- Calculate cost per successful job
- ROI analysis for LLM usage

## Risk Mitigation

1. **Token Costs**
   - Implement aggressive caching
   - Use smaller models where possible
   - Batch similar requests

2. **Latency**
   - Parallel tool execution where possible
   - Timeout handling
   - Fallback to mock for time-sensitive operations

3. **Reliability**
   - Robust error handling
   - Automatic retries with backoff
   - Circuit breaker pattern for LLM failures

4. **Backwards Compatibility**
   - Keep mock implementations
   - Feature flags for gradual rollout
   - A/B testing capabilities

## Success Metrics

1. **Functional Success**
   - Job completion rate > 80%
   - Correct answers for test cases
   - Tool reliability > 95%

2. **Performance Metrics**
   - Average job completion < 30s
   - Token usage within budget
   - Fallback rate < 10%

3. **Quality Metrics**
   - Summarization quality scores
   - Classification accuracy
   - Math problem solving rate

## Current Status (Updated)

**âœ… COMPLETED WORK:**
- Sprints 1-4 are fully complete
- LangChain integration foundation established
- stringKit tool enhanced with LLM capabilities (summarization & classification)
- calc tool completely rewritten with mathjs security and LLM word problem solving
- retrieval tool enhanced with vector embeddings and semantic search
- Production bug fixed (job completion status)
- Comprehensive test suite implemented

**ðŸ“‹ READY FOR NEXT PHASE:**
- Sprint 5: browser Tool Enhancement (LLM-powered content extraction)
- Sprint 6: Agent Integration and performance analysis

## Next Steps

1. âœ… Review and approve this plan (DONE)
2. âœ… Create feature branch: `feat/enhanced-retrieval-and-memory` (DONE)
3. âœ… Implement Step 1.1-1.2 (Dependencies and LLM Wrapper) (DONE)
4. âœ… Begin with stringKit enhancement (highest impact) (DONE)
5. âœ… Implement calc tool security and enhancement (DONE)
6. âœ… Complete Sprint 4 - retrieval Tool Intelligence (DONE)
7. **NEXT:** Begin Sprint 5 - browser Tool Enhancement

## References

- [LangChain.js Documentation](https://js.langchain.com/)
- [LangChain Tools Guide](https://js.langchain.com/docs/modules/agents/tools/)
- [MathJS Documentation](https://mathjs.org/)
- [Ollama Integration](https://js.langchain.com/docs/integrations/llms/ollama)