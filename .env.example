# Global/common
NODE_ENV=development

# ===============================
# Multi-Provider LLM Configuration
# ===============================

# Global LLM provider selection
LLM_PROVIDER=vertex  # Options: vertex, ollama, auto, '' (defaults to vertex)
DEFAULT_MODEL=gemini-1.5-flash  # Default model when not specified per-component
OLLAMA_URL=http://localhost:11434  # Ollama server endpoint

# Component-specific LLM configurations
# Format: "provider:model:temperature:maxTokens"
# Examples:
#   LLM_CONFIG_NAME_GENERATOR=vertex:gemini-1.5-pro:0.9:2000
#   LLM_CONFIG_JOB_GENERATOR=ollama:llama3.2:7b:0.8:1500
#   LLM_CONFIG_RESULT_GRADER=vertex:gemini-1.5-flash:0.1:1000
#   LLM_CONFIG_AGENT=auto:gemini-1.5-flash:0.7:
#   LLM_CONFIG_CODE_GENERATOR=vertex:gemini-1.5-pro:0.3:4000
#   LLM_CONFIG_TOOL_BUILDER=ollama:granite3.1-dense:8b:0.7:2000
#   LLM_CONFIG_SWARM_SYNTHESIZER=vertex:gemini-1.5-flash:0.7:

LLM_CONFIG_NAME_GENERATOR=
LLM_CONFIG_JOB_GENERATOR=
LLM_CONFIG_RESULT_GRADER=
LLM_CONFIG_AGENT=
LLM_CONFIG_CODE_GENERATOR=
LLM_CONFIG_SWARM_SYNTHESIZER=
LLM_CONFIG_TOOL_BUILDER=

# ===============================
# Legacy Configuration (backward compatibility)
# ===============================

# Google Vertex AI (required for vertex provider)
GOOGLE_CLOUD_PROJECT=
GOOGLE_CLOUD_LOCATION=us-central1
GOOGLE_APPLICATION_CREDENTIALS=
# Alternative: GOOGLE_CLOUD_CREDENTIALS for base64 encoded credentials

# Legacy Vertex AI Model Configuration (still supported)
VERTEX_AI_MODEL=gemini-1.5-flash  # Options: gemini-1.5-flash, gemini-1.5-pro, gemini-1.5-flash-8b
VERTEX_AI_TEMPERATURE=0.7         # 0.0 (deterministic) to 2.0 (very creative)

# Component-specific token limits (optional - undefined = no limit)
VERTEX_AI_MAX_OUTPUT_TOKENS=      # General default token limit
VERTEX_AI_MAX_OUTPUT_TOKENS_JOB_GENERATOR=
VERTEX_AI_MAX_OUTPUT_TOKENS_TOOL_BUILDER=
VERTEX_AI_MAX_OUTPUT_TOKENS_CODE_GENERATOR=
VERTEX_AI_MAX_OUTPUT_TOKENS_NAME_GENERATOR=
VERTEX_AI_MAX_OUTPUT_TOKENS_LLM_GRADER=
VERTEX_AI_MAX_OUTPUT_TOKENS_AGENT=

# Legacy token budgets (for backward compatibility)
LLM_MAX_TOKENS_PER_HOUR=100000
LLM_MAX_TOKENS_PER_AGENT=1000

# Local LLM via Ollama (required for ollama provider)
LOCAL_LLM_ENABLED=0  # Set to 1 to enable (legacy - use LLM_PROVIDER=ollama instead)
LOCAL_MODEL_PATH=granite3.1-dense:8b  # Default Ollama model
LOCAL_LLM_ENDPOINT=http://localhost:11434/api/generate  # Legacy endpoint format
LOCAL_LLM_MAX_TOKENS_PER_HOUR=200000
LOCAL_LLM_MAX_TOKENS_PER_AGENT=2000

# Services ports
SOUP_RUNNER_PORT=3000
BROWSER_GATEWAY_PORT=3100
SITE_KB_PORT=3200

# Browser gateway
ALLOWED_HOSTS=localhost,127.0.0.1,*.local,*.example.com

# MCP Knowledge Server (optional)
# MCP_KNOWLEDGE_SERVER=http://localhost:8080
# MCP_BEARER_TOKEN=your-bearer-token-here

# Runner (queues, db, costs)
REDIS_URL=redis://localhost:6379
DATABASE_URL=file:./dev.db
JOBS_PER_MIN=10
EPOCH_MINUTES=120
FAIL_PENALTY=3
BROWSER_STEP_COST=1

# Bootstrap mode for soup-runner (skip Redis/Prisma)
SOUP_BOOTSTRAP=1

# Model preloading configuration (for faster startup)
PRELOAD_MODELS=1                # Enable/disable model preloading (1/0, true/false)
PRELOAD_TIMEOUT_SECONDS=90      # Timeout per model in seconds
PRELOAD_RETRY_ATTEMPTS=2        # Number of retry attempts per model

# Tool Builder Agent Configuration
USE_LANGCHAIN_TOOL_BUILDER=true  # Use new LangChain-based ToolBuilderAgent (true) or legacy custom implementation (false)
